
```{r, echo = FALSE, include = FALSE}
source("./scripts/packages.R") # load necessary packages
source("./scripts/boxplot_legend.R")
df <- read.csv("./data/week_not_clean.csv", header = TRUE, sep = ",", dec = ".", check.names = FALSE)
```

# Data cleaning

Raw data are not always cleaned and ready for the analyst to be processed. Usually there are many anomalies that have to be fixed before the analytics process. Typical problems that can generate anomalies in the data:

- Meter communication failed;
- Sensor break or modification;
- Intervention on the system (e.g. revamping of the heating/cooling system);
- Modification of the building usage;
- Catastrophic events.

Thus it is necessary to undergo a data preparation process that is usually the most time consuming task but also the most important.

The process of "fixing" the data is called data cleaning and the main objective is to manage missing values, resolve inconsistencies, and detect and remove outliers.

```{r, echo = FALSE}

p1<- ggplot(data =  df,
            aes(x = as.POSIXct(Date_Time, "%Y-%m-%d %H:%M:%S", tz = "Europe/Rome"),
                y =  Total_Power,
                color = Type
            )
) + 
  geom_point(na.rm = TRUE) + 
  scale_x_datetime(
    breaks = date_breaks("8 hour"),                     
    labels = date_format(("%H:%M") , tz = "Europe/Rome"),
    expand = c(0,0)                                     
  ) +
  scale_color_manual(values = c("blue", "red", "gray", "green")) + 
  theme_classic() +
  ggplot2::theme(
    legend.text = element_text(size = 10),
    legend.position = "top",                     # legend position on the top of the graph
    legend.direction = "horizontal",             # layout of items in legends
    legend.box = "horizontal",                   # arrangement of multiple legends
    strip.text = element_text(size = 10), # facet wrap title fontsize
    axis.title.x = element_text(size = 12,margin = margin(t = 20, r = 20, b = 0, l = 0)),
    axis.title.y = element_text(size = 12,margin = margin(t = 20, r = 20, b = 0, l = 0)),
    axis.text.x = element_text(size = 10, angle = 45, vjust=.0),
    axis.text.y = element_text(size = 10)
  ) + 
  guides(colour=guide_legend("", override.aes=list(colour=c("blue", "red", "gray", "green") ))) + 
  labs( x = "Hour" , y = "Electrical Load [kW]", legend = "")                    # axis label

ggMarginal(p1, type="boxplot", color = "blue", outlier.color = "green", margins = "y")
```


## Inconsistences
## Outliers
A general definition of outlier is the following

> "Outliers are records that appear to deviate significantly from other elements in
the sample in which they occur. Outlier can be also defined as an observation (or subset of observations) that appears to be inconsistent with the remainder of that set of data."
>
> --- Enhancing energy efficiency in buildings through innovative data analytics technologies Capozzoli

In time series analysis the outliers are defied as:

> "Observations unlikely to occur given the variance of the observations of the rest of the time series 
>
> --- [C. Fan, F. Xiao, H. Madsen, and D. Wang, «Temporal knowledge discovery in big BAS data for building energy management», Energy and Buildings] . 

They can be distinguished into 

- Punctual outliers
- Sequence outliers.

In the data pre-processing phase, the identification of punctual outliers is needed, whenever infrequent sequential patterns could be detected through further investigation at a later stage.

### Inter-quartile method for punctual outlier identification

Detecting outliers with the interquartile method requires the calculation of three fundamental values:

- First quartile $Q_1$ : the point between the smallest value and the median (25% of the distribution) ;
- Second quartile or median $Q_2$ : the middle value of the dataset (50% of the distribution);
- Third quartile $Q_3$ : the point between median and the highest value (75% of the distribution);


The difference between the third and the first quartile is called Inter-Quartile Range $\text{IQR} = Q_3-Q_1$.

To detect the outliers is necessary to define an acceptability range and any data point lying outside this range are considered as outlier. The range is as given below:

\begin{equation}
R = [Q_2 − k \text{IQR}, Q_3 + k \text{IQR}]
\end{equation}

Where $k$ is an arbitrary parameters usually set at $1.5$. Any data point less than the Lower Bound or higher than the Upper Bound is considered as an *outlier*.

An effective way to visualize variables distributions and detect outliers is the boxplot (Section \@ref(sec:boxplots)) because it summarizes in a single plot all the statistics needed. Those statistics can be easily calculated through the R package `stats` as follows:

```{r}
Q1 = quantile(df$Total_Power, na.rm = T, c(0.25))  
Q2 = quantile(df$Total_Power, na.rm = T, c(0.50))  
Q3 = quantile(df$Total_Power, na.rm = T, c(0.75))  
Iqr = IQR(df$Total_Power, na.rm = T)               
k = 1.5
Lower_bound = Q1 - k*Iqr
Upper_bound = Q2 + k*Iqr
```

It is possible to identify the outliers by calculating the previous statistics and setting manually the range limits or by using the  `boxplot.stats` function which, given the $k$ coefficient, returns a list of values in which and `$out` is a vector of all the detected outliers. 
```{r}
outliers <- boxplot.stats(df$Total_Power, coef = k)$out
```

In the electrical load dataset, for the variable  `Total Power`,  the following values are outliers: `r paste(outliers,"kW")`). Once identified it can be removed by substituting NA value.

```{r}
df$Total_Power[match(outliers, df$Total_Power)] <- NA
```



### Z-score method for punctual outlier identification
Z-scores can quantify the unusualness of an observation when your data follow the normal distribution. Z- scores are the number of standard deviations above and below the mean that each value falls. A standard cut-off value for finding outliers are Z-scores of +/-3 or further from zero.

A given time series y(t) = {y1, . . . yn} of length n with mean μ and standard deviation σ is transformed into a new time series Z(t) = {Z1, . . . Zn} of length n with zero mean μ = 0 and unitary standard deviation σ = 1 through the equation (2.1).
Z(t) = y(t) − μ (2.1) σ
This process allows simplifying the analysis through the use of Z-scores, which is a measure of the position of data and represents how many standard deviations it is far from the mean of a standard normal distribution N(0,1). If the value is positive, the value lies above the mean; if negative it lies below. Z-scores allows to easily calculate the area under a normal Gaussian distribution and will be useful when dividing the distribution into equally probable areas.

\begin{equation}
\frac{y-y_1}{y_2-y_1}  = \frac{x-x_1}{x_2-x_1}
\end{equation}


## Missing values

Common methods to handle missing values are global constant, local constant, moving average and interpolation.

### Global constant method 

substitute ALL the missing values in the data with a single value (e.g. Mean or Median value of the data).

### Local constant method 

substitute the missing values in the data with a values calculated locally. This process is carried out through Lookup Tables.

```{r}
data.frame(Day = c("Mon", "Tue"), Mean = c(1,2))
```


### Moving Average or Simple Moving Average (SMA)

method substitute missing values with the unweighted mean of N previous valid measures.
$$
\begin{equation}
y = \frac{1}{N}\sum_{i=1}^n{y_{-i}}
\end{equation}
$$


### Linear interpolation method
substitute missing values with the values that lies on the straight line between two valid measures [(x0,y0),(x1,y1)].

$$
\begin{equation}
\frac{y-y_1}{y_2-y_1}  = \frac{x-x_1}{x_2-x_1}
\end{equation}
$$