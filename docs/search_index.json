[["index.html", "Data mining methods for building energy data Author", " Data mining methods for building energy data Roberto Chiosa 2020-12-30 Author "],["building-related-data.html", "Chapter 1 Building related data 1.1 influencing variables 1.2 Time series 1.3 Visualizations", " Chapter 1 Building related data Data are usually described by a set of features also called variables, attributes or dimensions. Categorical: Features whose values are taken from a defined set of values. Numerical: Features whose values are continuous or integer-valued. They are represented by numbers and possess most of the properties of numbers. continuous discrete categorical variables numerical variables whole building meter level system level dynamic static 1.1 influencing variables 1.2 Time series 1.3 Visualizations 1.3.1 Box plots p1&lt;- df %&gt;% ggplot() + geom_histogram(aes(x = Total_Power, y = ..density..), colour = &quot;black&quot;, fill = &quot;white&quot;, binwidth = 40, na.rm = T) + geom_density(aes(x = Total_Power, y = ..density..), alpha = .2, fill = &quot;#FF6666&quot;, na.rm = T) + stat_boxplot(aes(x = Total_Power, y = -0.0025), geom =&#39;errorbar&#39;, width = 0.0025) + geom_boxplot(aes(x = Total_Power, y = -0.0025), fill = &quot;lightgrey&quot;, width = 0.0025, na.rm = T) + geom_point(aes(x = mean(Total_Power, na.rm = T), y = -0.0025), shape = 3, size = 1.5) + labs( x = &quot;Electrical load [kW]&quot; , y = &quot;Density [-]&quot;) + theme_classic() + coord_flip() p2 &lt;- ggplot_box_legend() ggarrange(p1, p2, nrow = 1) ## Warning: Removed 82 rows containing non-finite values (stat_boxplot). 1.3.2 scatter plots 1.3.3 carpet plots 1.3.4 calendar plots "],["data-cleaning.html", "Chapter 2 Data cleaning 2.1 Inconsistences 2.2 Outliers 2.3 Missing values", " Chapter 2 Data cleaning Raw data are not always cleaned and ready for the analyst to be processed. Usually there are many anomalies that have to be fixed before the analytics process. Typical problems that can generate anomalies in the data: Meter communication failed; Sensor break or modification; Intervention on the system (e.g. revamping of the heating/cooling system); Modification of the building usage; Catastrophic events. Thus it is necessary to undergo a data preparation process that is usually the most time consuming task but also the most important. The process of “fixing” the data is called data cleaning and the main objective is to manage missing values, resolve inconsistencies, and detect and remove outliers. 2.1 Inconsistences 2.2 Outliers A general definition of outlier is the following “Outliers are records that appear to deviate significantly from other elements in the sample in which they occur. Outlier can be also defined as an observation (or subset of observations) that appears to be inconsistent with the remainder of that set of data.” — Enhancing energy efficiency in buildings through innovative data analytics technologies Capozzoli In time series analysis the outliers are defied as: \"Observations unlikely to occur given the variance of the observations of the rest of the time series — [C. Fan, F. Xiao, H. Madsen, and D. Wang, «Temporal knowledge discovery in big BAS data for building energy management», Energy and Buildings] . They can be distinguished into Punctual outliers Sequence outliers. In the data pre-processing phase, the identification of punctual outliers is needed, whenever infrequent sequential patterns could be detected through further investigation at a later stage. 2.2.1 Inter-quartile method for punctual outlier identification Detecting outliers with the interquartile method requires the calculation of three fundamental values: First quartile \\(Q_1\\) : the point between the smallest value and the median (25% of the distribution) ; Second quartile or median \\(Q_2\\) : the middle value of the dataset (50% of the distribution); Third quartile \\(Q_3\\) : the point between median and the highest value (75% of the distribution); The difference between the third and the first quartile is called Inter-Quartile Range \\(\\text{IQR} = Q_3-Q_1\\). To detect the outliers is necessary to define an acceptability range and any data point lying outside this range are considered as outlier. The range is as given below: \\[\\begin{equation} R = [Q_2 − k \\text{IQR}, Q_3 + k \\text{IQR}] \\end{equation}\\] Where \\(k\\) is an arbitrary parameters usually set at \\(1.5\\). Any data point less than the Lower Bound or higher than the Upper Bound is considered as an outlier. An effective way to visualize variables distributions and detect outliers is the boxplot (Section 1.3.1) because it summarizes in a single plot all the statistics needed. Those statistics can be easily calculated through the R package stats as follows: Q1 = quantile(df$Total_Power, na.rm = T, c(0.25)) Q2 = quantile(df$Total_Power, na.rm = T, c(0.50)) Q3 = quantile(df$Total_Power, na.rm = T, c(0.75)) Iqr = IQR(df$Total_Power, na.rm = T) k = 1.5 Lower_bound = Q1 - k*Iqr Upper_bound = Q2 + k*Iqr It is possible to identify the outliers by calculating the previous statistics and setting manually the range limits or by using the boxplot.stats function which, given the \\(k\\) coefficient, returns a list of values in which and $out is a vector of all the detected outliers. outliers &lt;- boxplot.stats(df$Total_Power, coef = k)$out In the electrical load dataset, for the variable Total Power, the following values are outliers: 1933 kW, 1200 kW). Once identified it can be removed by substituting NA value. df$Total_Power[match(outliers, df$Total_Power)] &lt;- NA 2.2.2 Z-score method for punctual outlier identification Z-scores can quantify the unusualness of an observation when your data follow the normal distribution. Z- scores are the number of standard deviations above and below the mean that each value falls. A standard cut-off value for finding outliers are Z-scores of +/-3 or further from zero. A given time series y(t) = {y1, . . . yn} of length n with mean μ and standard deviation σ is transformed into a new time series Z(t) = {Z1, . . . Zn} of length n with zero mean μ = 0 and unitary standard deviation σ = 1 through the equation (2.1). Z(t) = y(t) − μ (2.1) σ This process allows simplifying the analysis through the use of Z-scores, which is a measure of the position of data and represents how many standard deviations it is far from the mean of a standard normal distribution N(0,1). If the value is positive, the value lies above the mean; if negative it lies below. Z-scores allows to easily calculate the area under a normal Gaussian distribution and will be useful when dividing the distribution into equally probable areas. \\[\\begin{equation} \\frac{y-y_1}{y_2-y_1} = \\frac{x-x_1}{x_2-x_1} \\end{equation}\\] 2.3 Missing values Common methods to handle missing values are global constant, local constant, moving average and interpolation. 2.3.1 Global constant method substitute ALL the missing values in the data with a single value (e.g. Mean or Median value of the data). 2.3.2 Local constant method substitute the missing values in the data with a values calculated locally. This process is carried out through Lookup Tables. data.frame(Day = c(&quot;Mon&quot;, &quot;Tue&quot;), Mean = c(1,2)) ## Day Mean ## 1 Mon 1 ## 2 Tue 2 2.3.3 Moving Average or Simple Moving Average (SMA) method substitute missing values with the unweighted mean of N previous valid measures. \\[ \\begin{equation} y = \\frac{1}{N}\\sum_{i=1}^n{y_{-i}} \\end{equation} \\] 2.3.4 Linear interpolation method substitute missing values with the values that lies on the straight line between two valid measures [(x0,y0),(x1,y1)]. \\[ \\begin{equation} \\frac{y-y_1}{y_2-y_1} = \\frac{x-x_1}{x_2-x_1} \\end{equation} \\] "],["data-transformation.html", "Chapter 3 data transformation 3.1 data scaling", " Chapter 3 data transformation The process of data transformation consists of scaling data and data type transformation. In a typical building related data set the scale of variables can vary largely due to different units used. The power demand may change from 0 kW to thousands of kW, the temperature (eg, outdoor temperature, indoor temperature, supply temperature of the system) can range from − 20°C to °C or a typical signal of a fan can range from 0 to 1. 3.1 data scaling The purpose of data scaling is to normalize the data variables so that they end up being equally important in data analysis from a quantity point of view. The methods used for data scaling include max-min normalization, Z-score normalization, and decimal point normalization ## data type transformation ## data reduction ###PAA 3.1.1 data segmentation The segmentation (or summarization) task aims at creating an accurate approximation of time series, by reducing its dimensionality while retaining its essential features. "],["unsupervised-learning.html", "Chapter 4 unsupervised learning 4.1 clustering 4.2 ARM", " Chapter 4 unsupervised learning 4.1 clustering 4.1.1 partitive 4.1.2 hierarchical 4.1.3 density 4.2 ARM "],["supervised-learning.html", "Chapter 5 supervised learning 5.1 classification 5.2 regression 5.3 forecasting 5.4 benchmarking 5.5 artificial neural network", " Chapter 5 supervised learning 5.1 classification 5.2 regression 5.3 forecasting 5.4 benchmarking 5.5 artificial neural network "]]
